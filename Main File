import nltk 
nltk.download ('all')
import requests 
from bs4 import BeautifulSoup
form nltk.tokenize import word_tockenize 
from nltk.corpus import stopwords 
import re 

#Scrapping The 2 Websites and Combining them together 
def Scrape_Website(url) :
    response = requests.get(url)
    soup= BeautifulSoup (response.text , 'html.parser')
    paragraphs = soup.find_all ('p')
    return ' '.join([p.get_text() for p in paragraphs])
    

text1 = Scrape_Website ('https://en.wikipedia.org/wiki/Artificial_intelligence')
text2= Scrape_Website ('https://en.wikipedia.org/wiki/Machine_learning')
combined_text =  text1 + ' ' + text2

#Tockenization

#Lowercasing 

#Stopword Removal

#Removing Special Characters,Numbers and Punctuation
tokens = [re.sub(r'[^a-z]','', word )for word in tokens]
tokens = [word for word in tokens if word != '']
print ("After removing special characters, numbers and punctuation" , tokens [:10]) 

 

